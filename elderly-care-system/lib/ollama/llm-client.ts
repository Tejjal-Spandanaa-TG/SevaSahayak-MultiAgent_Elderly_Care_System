/**
 * Ollama LLM Client for SevaSahayak
 *
 * This client provides an interface to interact with Ollama LLMs running on-premise.
 * It supports text generation, chat, and other LLM capabilities.
 */

export interface OllamaRequestOptions {
  model: string
  prompt: string
  system?: string
  temperature?: number
  topP?: number
  topK?: number
  maxTokens?: number
  stream?: boolean
}

export interface OllamaResponse {
  model: string
  created_at: string
  response: string
  done: boolean
  context?: number[]
  total_duration?: number
  load_duration?: number
  prompt_eval_duration?: number
  eval_count?: number
  eval_duration?: number
}

export class OllamaLLMClient {
  private baseUrl: string
  private defaultModel: string

  constructor(baseUrl = "http://localhost:11434", defaultModel = "mistral:7b") {
    this.baseUrl = baseUrl
    this.defaultModel = defaultModel
  }

  /**
   * Generate text using the Ollama API
   */
  async generateText(options: Partial<OllamaRequestOptions>): Promise<OllamaResponse> {
    const requestOptions: OllamaRequestOptions = {
      model: options.model || this.defaultModel,
      prompt: options.prompt || "",
      system: options.system,
      temperature: options.temperature,
      topP: options.topP,
      topK: options.topK,
      maxTokens: options.maxTokens,
      stream: options.stream || false,
    }

    try {
      // In a real implementation, this would call the Ollama API
      // For demo purposes, we're simulating the response
      console.log(`Calling Ollama model: ${requestOptions.model}`)
      console.log(`System prompt: ${requestOptions.system || "None"}`)
      console.log(`User prompt: ${requestOptions.prompt}`)

      // Simulate API call delay
      await new Promise((resolve) => setTimeout(resolve, 500))

      // Mock response
      return {
        model: requestOptions.model,
        created_at: new Date().toISOString(),
        response: `This is a simulated response from the ${requestOptions.model} model. In a real implementation, this would be generated by Ollama.`,
        done: true,
        context: [1, 2, 3, 4], // Mock context
        total_duration: 1250000000, // Mock duration in nanoseconds
        load_duration: 400000000,
        prompt_eval_duration: 650000000,
        eval_count: 40,
        eval_duration: 200000000,
      }
    } catch (error) {
      console.error("Error calling Ollama:", error)
      throw new Error(`Failed to call Ollama: ${error.message}`)
    }
  }

  /**
   * Stream text generation from Ollama
   */
  async *streamText(options: Partial<OllamaRequestOptions>): AsyncGenerator<string, void, unknown> {
    const requestOptions = {
      ...options,
      stream: true,
      model: options.model || this.defaultModel,
    }

    try {
      // In a real implementation, this would stream from the Ollama API
      // For demo purposes, we're simulating the streaming response
      console.log(`Streaming from Ollama model: ${requestOptions.model}`)

      const mockResponses = [
        "I'll analyze ",
        "the health data ",
        "for any anomalies. ",
        "Based on the vital signs, ",
        "everything appears normal. ",
        "No immediate concerns detected.",
      ]

      for (const chunk of mockResponses) {
        await new Promise((resolve) => setTimeout(resolve, 200))
        yield chunk
      }
    } catch (error) {
      console.error("Error streaming from Ollama:", error)
      throw new Error(`Failed to stream from Ollama: ${error.message}`)
    }
  }

  /**
   * Get available models from Ollama
   */
  async getAvailableModels(): Promise<string[]> {
    try {
      // In a real implementation, this would call the Ollama API to list models
      // For demo purposes, we're returning a fixed list
      return ["mistral:7b", "llama2:7b", "llama2:13b", "codellama:7b", "orca-mini:7b"]
    } catch (error) {
      console.error("Error getting available models:", error)
      throw new Error(`Failed to get available models: ${error.message}`)
    }
  }
}

// Create and export a singleton instance
export const ollamaClient = new OllamaLLMClient()

