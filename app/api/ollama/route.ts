import { NextResponse } from "next/server"

// This is a simplified API route for interacting with Ollama
// In a real implementation, this would use the Ollama API

export async function POST(request: Request) {
  try {
    const body = await request.json()
    const { model, prompt, system } = body

    // Validate request
    if (!model || !prompt) {
      return NextResponse.json({ error: "Model and prompt are required" }, { status: 400 })
    }

    // In a real implementation, this would call the Ollama API
    console.log(`Calling Ollama model: ${model}`)
    console.log(`System prompt: ${system || "None"}`)
    console.log(`User prompt: ${prompt}`)

    // Mock response
    const response = {
      model: model,
      created_at: new Date().toISOString(),
      response: `This is a mock response from the ${model} model. In a real implementation, this would be generated by Ollama.`,
      done: true,
      context: [1, 2, 3, 4], // Mock context
      total_duration: 1250000000, // Mock duration in nanoseconds
      load_duration: 400000000,
      prompt_eval_duration: 650000000,
      eval_count: 40,
      eval_duration: 200000000,
    }

    return NextResponse.json(response)
  } catch (error) {
    console.error("Error calling Ollama:", error)
    return NextResponse.json({ error: "Failed to call Ollama" }, { status: 500 })
  }
}

